# -*- coding: utf-8 -*-
"""Interest_rate_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h2tSgms1f-kqzcKDVcDs1HzGXeenZIQd
"""

import pandas as pd
import numpy as np
data = pd.read_csv("loans_full_schema.csv")

#data info
data.info()

#dropping 'emp_title' , 'delinq_2y' , 'inquiries_last_12m', 'account_never_delinq_percent', 'application_type', "grade"
#as these features are irrelevant or reduntant
data = data.drop(columns =['emp_title' , 'num_accounts_120d_past_due','delinq_2y' , 'inquiries_last_12m', 'account_never_delinq_percent', 'application_type', "grade" ])

#data head
data.head()

#percentage of missing values in each column
data.isnull().sum()/10000 * 100

#dropping months_since_90d_late , annual_income_joint, verification_income_joint and 
#debt_to_income_joint as it contains more than 60 % missing values
data = data.drop(columns=['months_since_90d_late','annual_income_joint','verification_income_joint','debt_to_income_joint'])

data.info()

#plotting a heat map to see the correlation between the features
import matplotlib.pyplot as plt
import seaborn as sns
corrs = data.corr()
plt.figure(figsize = (30, 20))
sns.heatmap(corrs, annot = True)

"""From the correlation map we can see that the features: 'current_accounts_delinq','num_accounts_30d_past_due','paid_principal' is not much correlated with the target "interest_rate""""

#dropping the above mentioned features
data= data.drop(columns=['current_accounts_delinq','num_accounts_30d_past_due','paid_principal'])

#histogram 
data.hist(figsize=(20,20),color = 'black')

"""The above graphs show the distribution of each column. From these graphs we can see that some features are not uniformly distrubted """

#checking the distribution of some unevenly distributed features
data['total_collection_amount_ever'].value_counts()

data['paid_late_fees'].value_counts()

data['tax_liens'].value_counts()

data['num_collections_last_12m'].value_counts()

data['num_historical_failed_to_pay'].value_counts()

#dropping the above mentioned features as these wont contribute much to prediction
data= data.drop(columns=['total_collection_amount_ever','paid_late_fees','tax_liens','num_collections_last_12m','num_historical_failed_to_pay'])

#boxplot
data.boxplot(grid=True, rot=90, fontsize=15,figsize=(20,10))

"""From the above graphs we can see that some of the features contains outliers."""

#deleting outliers
for cols in data.columns:
    if data[cols].dtype == 'int64' or data[cols].dtype == 'float64':
        upper_range = data[cols].mean() + 3 * data[cols].std()
        lower_range = data[cols].mean() - 3 * data[cols].std()
        
        indexs = data[(data[cols] > upper_range) | (data[cols] < lower_range)].index
        data = data.drop(indexs)

data.shape

data.info()

#filling the missing values
data.isnull().sum()

#plotting heat map of missing values
import missingno as msno
msno.heatmap(data)

#filling missing values of emp_length
data['emp_length'].value_counts()

data['emp_length'].fillna(10,inplace=True)

#filling missing values of months_since_last_delinq
data['months_since_last_delinq'].value_counts()

mean = data['months_since_last_delinq'].mean()
data['months_since_last_delinq'].fillna(mean,inplace=True)

#filling missing values of debt_to_income
data['debt_to_income'].value_counts()

meandebt = data['debt_to_income'].mean()
data['debt_to_income'].fillna(meandebt,inplace=True)

#filling missing values of months_since_last_credit_inquiry
data['months_since_last_credit_inquiry'].value_counts()

data['months_since_last_credit_inquiry'].fillna(1,inplace=True)

data.isnull().sum()

#convert categorical values into numerical ones
columns = []
for cols in data.columns:
    if data[cols].dtype == 'object':
        columns.append(cols)
        
columns

dummies_feature = pd.get_dummies(data[columns])
dummies_feature.head()

data = pd.concat([data, dummies_feature], axis=1)
data.head()

data.drop(['state',
 'homeownership',
 'verified_income',
 'loan_purpose',
 'sub_grade',
 'issue_month',
 'loan_status',
 'initial_listing_status',
 'disbursement_method'],axis=1,inplace = True)

#let's see the distribution of our target value "interest_rate"
data['interest_rate'].min()

data['interest_rate'].max()

bins=[5,10,15,20,25,30]
out=pd.cut(data.interest_rate,
          bins=bins,
          include_lowest=True)
ax=out.value_counts(sort=False).plot.bar(
rot=0,
color='g',   
    
figsize=(10,10))
plt.xlabel('interest_rate')
plt.ylabel('count')
plt.show

#splitting the data into test and train
y = data['interest_rate']
X = data.copy()
X = X.drop(columns=['interest_rate'])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

#RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics

def RandomForest(xtrain, ytrain, xtest, ytest):
    randomForest = RandomForestRegressor()
    randomForest.fit(xtrain, ytrain)
    y_pred = randomForest.predict(xtest)
    
    #accurracy messaures    
    print('MAE:', metrics.mean_absolute_error(ytest, y_pred))
    print('MSE:', metrics.mean_squared_error(ytest, y_pred))
    print('R2_score:', metrics.r2_score(ytest, y_pred))
    #plot of y pred
    plt.scatter(y_pred,ytest)
    plt.xlabel('predicted value of y')
    plt.ylabel('y')
    plt.figure()

RandomForest(X_train, y_train, X_test, y_test)

#gradient boosting
from sklearn.ensemble import GradientBoostingRegressor

#maxdepth: 20, minsamleaf: 117, n: 73, maxfeat: 10, lr: 0.07
def gradientboostingmachine(md, msl, n, mf, lr, X_train, y_train, X_test, y_test):
    gbm_best = GradientBoostingRegressor(n_estimators=n, random_state=1111,
                                         max_depth=md, max_features=mf, 
                                         min_samples_leaf=msl, learning_rate=lr
                                         )
    gbm_best.fit(X_train, y_train)
    y_pred_gbm = gbm_best.predict(X_test)
   
    print('MAE:', metrics.mean_absolute_error(y_test, y_pred_gbm))
    print('MSE:', metrics.mean_squared_error(y_test, y_pred_gbm))
    print('R2_score:', metrics.r2_score(y_test, y_pred_gbm))

    plt.scatter(y_pred_gbm,y_test)
    plt.xlabel('predicted value of y')
    plt.ylabel('y')
    plt.figure()
gradientboostingmachine(20, 117, 73, 10, 0.07, X_train, y_train, X_test, y_test)

"""As it was regression problem I used Random Forest Regressor and Gradient Boosting Regressor as my model and Random Forest Regressor perfomed really well than gradient Boosting. I think the model accuracy can be further improved with hypertuning parameters. Also, if I had more time, I would have looked into each feature and done further datapreprocessing and also created a wide and deep model."""